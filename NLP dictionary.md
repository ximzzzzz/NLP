## NLP dictionary



### 말뭉치

임베딩 학습이라는 목적을 가지고 수집한 표본 전체. 위키백과와 나무위키의 문장을 수집했을때 

위키백과+나무위키 = 말뭉치



### 컬렉션

말뭉치에 속한 각각의 집합. 위키백과, 나무위키 각각은 컬렉션이라 할 수 있다.



### 토큰

학습에 쓰이는 가장작은 단위. 일반적으로 단어, 형태소



### 어휘집합

말뭉치에 있는 모든 문장을 토크나이즈를 실시한 후 중복을 제거한 토큰들의 집합



### Stopwords

전처리 과정에서 조사, 전치사와 같이 큰 의미는 없지만 너무 (자주 나오는 + 일반적인) 형태소를 날리는 작업

(반대로 빈도가 거의없는 단어를 날리는 작업도 포함된다)



### Stemming

전처리 과정에서 같은 어원에서 나온 합성어?( or 파생어?)를 하나의 어원이 되는 단어로 합치는 작업

> running, runs, ran 은 모두 run 이라는 단어로 통일할 수 있다(하지만 시제 정보를 잃을 수 있음)



### 잠재의미분석(Latent Semantic analysis)

단어 사용빈도 등 말뭉치의 통계량 정보가 들어있는 커다란 행렬에 **특이값분해**등 수학적 기법을 적용해 

데이터 차원수를 줄여 계산효율성을 키우고 숨겨진 잠재의미를 이끌어낼 수 있다.



### 행렬분해(factorization)기반 방법

말뭉치 정보가 들어있는 원래 행렬을 두 개 이상의 작은 행렬로 쪼개는 방식의 임베딩 기법

**잠재의미분석, GloVe, Swivel** 등이 속함



### 예측기반방법

특정 단어 주변에 어떤 단어가 나타날지 예측하거나 이전 단어가 주어졌을때 다음단어를 예측하거나,

문장 내 일부 단어를 지우고 해당단어가 무엇인지 맞추는 과장에서 학습하는 방법

**Word2vec, FastText, BERT, ELMo, GPT** 등이 여기에 속한다.



### 토픽기반방법

주어진 문제에 잠재된 주제를 추론하는 방식. **잠재디리클레할당(Latent Dirichlet Allocation)**이 대표적인 기법



### 백오브워즈(bag of words)

어떤단어가 많이 쓰였는지에 대한 정보를 중점. 단어의 순서 무시



### 언어모델(Language Model)

문장이 어떤 순서로 쓰였는가. 문장이 얼마나 자연스러운가에 중점. 단어의 순서 중시



### 분포가정(distributional assumption)

문장에서 어떤단어가 같이 쓰였는지에 중점



### 점별상호정보량(PMI : Pointwise mutual information)

두 확률변수 사이의 상관성을 계량화하는 단위. 두 확률변수가 독립일경우 0. 



### Fine tuning (approach)

최적화를 위해 pretrained model을 사용할 시 다운스트림 task 에 맞춰 레이어를 추가하고 

최적화 과정에서 pretrained model까지 함께 학습하는 방식 

> pretrained model 의 파라미터가  task에 맞춰 추가 학습을 진행한다.

### Feature-based (approach)

최적화를 위해 pretrained model을 사용할 시 다운스트림 task에 맞춰 추가되는 레이어는 학습되고

pretrained model 의 기존 학습된 파라미터는 유지하는 방식



### GloVe

word2vec 과 잠재의미분석 각각의 단점을 개선시킨 모델 각 단점은 아래와 같다

- word2vec : 단어간 유사도를 측정할 수 있지만 윈도우 내에 문맥만 학습하기 때문에 말뭉치 전체의 통계정보는

  ​					반영되지 않는다.

- 잠재의미분석 : 말뭉치 전체의 통계량을 활용하지만 그 결과물로 단어간 유사도를 측정하기 어렵다.

정리하자면, 임베딩된 단어 간 유사도 측정을 수월하게 하면서도 말뭉치 전체의 통계정보를 더 반영하려는 모델

임베딩된 두 단어 벡터의 내적이 말뭉치 전체에서 두단어가 동시등장 빈도의 로그값이 되도록 목적함수 설정함



### Swivel

행렬분해기반의 단어 임베딩기법. GloVe는 단어-문맥행렬을 분해했지만, Swivel은 PMI행렬을 분해한다는 점에서 

차이가 있다.